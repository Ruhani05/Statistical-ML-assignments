{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "tKszhjELBBDm"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L07NN8XaAURH",
        "outputId": "7b0595bd-9fc8-41b0-ba28-b8dda482c7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction for new person (Age=42, Income=Low, Student=No, Credit=Excellent): Buy\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset\n",
        "data = pd.DataFrame({\n",
        "    'Age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
        "    'Income': ['High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High'],\n",
        "    'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No'],\n",
        "    'Credit': ['Fair', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair'],\n",
        "    'Buy Computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No']\n",
        "})\n",
        "\n",
        "# Encode categorical features as numerical values\n",
        "data_encoded = data.copy()\n",
        "data_encoded['Income'] = data_encoded['Income'].map({'High': 0, 'Medium': 1, 'Low': 2})\n",
        "data_encoded['Student'] = data_encoded['Student'].map({'No': 0, 'Yes': 1})\n",
        "data_encoded['Credit'] = data_encoded['Credit'].map({'Fair': 0, 'Excellent': 1})\n",
        "data_encoded['Buy Computer'] = data_encoded['Buy Computer'].map({'No': 0, 'Yes': 1})\n",
        "\n",
        "# Features and target variable\n",
        "X = data_encoded[['Age', 'Income', 'Student', 'Credit']].values\n",
        "y = data_encoded['Buy Computer'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train Decision Tree\n",
        "tree = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=1, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict for a new person (Age=42, Income=Low, Student=No, Credit=Excellent)\n",
        "new_person = [[42, 2, 0, 1]]  # Low Income (2), No Student (0), Excellent Credit (1)\n",
        "prediction = tree.predict(new_person)\n",
        "print(f'Prediction for new person (Age=42, Income=Low, Student=No, Credit=Excellent): {\"Buy\" if prediction[0] == 1 else \"No Buy\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVV0rJ-4Aao9",
        "outputId": "788016e2-96a6-4ea7-d6f6-b86e6161b69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging Model: 50.00%\n",
            "OOB Error of Bagging Model: 16.67%\n",
            "Prediction for new person: Buy\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Bagging with 10 trees\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=1),\n",
        "                           n_estimators=10, random_state=42, oob_score=True)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Compute OOB error\n",
        "oob_error = 1 - bagging.oob_score_\n",
        "\n",
        "print(f'Accuracy of Bagging Model: {accuracy * 100:.2f}%')\n",
        "print(f'OOB Error of Bagging Model: {oob_error * 100:.2f}%')\n",
        "new_sample = np.array([[42, 2, 0, 1]])  # Age=42, Income=Low, Student=No, Credit=Excellent\n",
        "pred = bagging.predict(new_sample)\n",
        "print(f'Prediction for new person: {\"Buy\" if pred[0] == 1 else \"No Buy\"}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHyWBwNQA1UZ",
        "outputId": "a4b2bdc4-0852-407a-934d-ae74937a9a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bagging with Random Features: 50.00%\n",
            "OOB Error of Bagging with Random Features: 16.67%\n",
            "Prediction for new person: Buy\n"
          ]
        }
      ],
      "source": [
        "# Bagging with 10 trees, using only two random predictors\n",
        "bagging_random_features = BaggingClassifier(estimator=DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=1),\n",
        "                                            n_estimators=10, max_features=2, random_state=42, oob_score=True)\n",
        "bagging_random_features.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_random_features = bagging_random_features.predict(X_test)\n",
        "accuracy_random_features = accuracy_score(y_test, y_pred_random_features)\n",
        "\n",
        "# Compute OOB error\n",
        "oob_error_random_features = 1 - bagging_random_features.oob_score_\n",
        "\n",
        "print(f'Accuracy of Bagging with Random Features: {accuracy_random_features * 100:.2f}%')\n",
        "print(f'OOB Error of Bagging with Random Features: {oob_error_random_features * 100:.2f}%')\n",
        "new_sample = np.array([[42, 2, 0, 1]])  # Age=42, Income=Low, Student=No, Credit=Excellent\n",
        "pred = bagging.predict(new_sample)\n",
        "print(f'Prediction for new person: {\"Buy\" if pred[0] == 1 else \"No Buy\"}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXX5powmA9Hv"
      },
      "source": [
        "Without using sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without scklearn \n",
        "Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZvTlSjWDsZX"
      },
      "source": [
        "Q3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBwf0WHPC_7O",
        "outputId": "309df83d-42bd-4af4-ffb4-c17832e45190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded Data:\n",
            " [[25  0  0  0  0]\n",
            " [30  0  0  1  0]\n",
            " [35  1  0  0  1]\n",
            " [40  2  0  0  1]\n",
            " [45  2  1  0  1]\n",
            " [50  2  1  1  0]\n",
            " [55  1  1  1  1]\n",
            " [60  0  0  0  0]]\n",
            "Encoded Features:\n",
            " [[25  0  0  0]\n",
            " [30  0  0  1]\n",
            " [35  1  0  0]\n",
            " [40  2  0  0]\n",
            " [45  2  1  0]\n",
            " [50  2  1  1]\n",
            " [55  1  1  1]\n",
            " [60  0  0  0]]\n",
            "Encoded Target:\n",
            " [0 0 1 1 1 0 1 0]\n",
            "Prediction for new person: Buy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset\n",
        "data = np.array([\n",
        "    [25, 'High', 'No', 'Fair', 'No'],\n",
        "    [30, 'High', 'No', 'Excellent', 'No'],\n",
        "    [35, 'Medium', 'No', 'Fair', 'Yes'],\n",
        "    [40, 'Low', 'No', 'Fair', 'Yes'],\n",
        "    [45, 'Low', 'Yes', 'Fair', 'Yes'],\n",
        "    [50, 'Low', 'Yes', 'Excellent', 'No'],\n",
        "    [55, 'Medium', 'Yes', 'Excellent', 'Yes'],\n",
        "    [60, 'High', 'No', 'Fair', 'No']\n",
        "])\n",
        "\n",
        "# Mapping for the categorical columns only\n",
        "feature_map = {\n",
        "    'High': 0, 'Medium': 1, 'Low': 2, 'No': 0, 'Yes': 1,\n",
        "    'Fair': 0, 'Excellent': 1\n",
        "}\n",
        "\n",
        "# Manually encode categorical columns and leave numerical columns (age) as they are\n",
        "def encode_data(data):\n",
        "    encoded_data = []\n",
        "\n",
        "    for row in data:\n",
        "        encoded_row = []\n",
        "        # Encode categorical features using the feature_map\n",
        "        for i, value in enumerate(row[1:]):  # Skip the age column\n",
        "            if value in feature_map:\n",
        "                encoded_row.append(feature_map[value])\n",
        "            else:\n",
        "                encoded_row.append(value)  # For age (numeric), leave it as it is\n",
        "        encoded_data.append([int(row[0])] + encoded_row)  # Add the age column back as it is\n",
        "\n",
        "    return np.array(encoded_data)\n",
        "\n",
        "# Encode the data\n",
        "data_encoded = encode_data(data)\n",
        "\n",
        "# Check the encoded data\n",
        "print(\"Encoded Data:\\n\", data_encoded)\n",
        "\n",
        "# Extract features and target\n",
        "X = data_encoded[:, :-1]  # Features (age, income, student, credit)\n",
        "y = data_encoded[:, -1]   # Target (buy computer)\n",
        "\n",
        "# Check if X and y are encoded correctly\n",
        "print(\"Encoded Features:\\n\", X)\n",
        "print(\"Encoded Target:\\n\", y)\n",
        "\n",
        "# Ensure X and y are numpy arrays and their types are correct\n",
        "X = np.array(X, dtype=int)\n",
        "y = np.array(y, dtype=int)\n",
        "\n",
        "# Decision Tree Implementation\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.tree = None\n",
        "\n",
        "    def gini_impurity(self, y):\n",
        "        class_labels = np.unique(y)\n",
        "        impurity = 1\n",
        "        for label in class_labels:\n",
        "            prob = np.sum(y == label) / len(y)\n",
        "            impurity -= prob ** 2\n",
        "        return impurity\n",
        "\n",
        "    def split_dataset(self, X, y, feature_index, threshold):\n",
        "        left_mask = X[:, feature_index] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        best_gini = float('inf')\n",
        "        best_split = None\n",
        "        num_features = X.shape[1]\n",
        "\n",
        "        for feature_index in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                X_left, X_right, y_left, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
        "\n",
        "\n",
        "                if len(y_left) == 0 or len(y_right) == 0:\n",
        "                    continue\n",
        "\n",
        "                gini_left = self.gini_impurity(y_left)\n",
        "                gini_right = self.gini_impurity(y_right)\n",
        "                gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_split = (feature_index, threshold)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        if len(np.unique(y)) == 1:  # If only one class remains, stop splitting\n",
        "            return {'label': y[0]}\n",
        "\n",
        "        if len(y) <= self.min_samples_leaf or (self.max_depth and depth >= self.max_depth):\n",
        "            return {'label': np.bincount(y).argmax()}\n",
        "\n",
        "        feature_index, threshold = self.best_split(X, y)\n",
        "        if feature_index is None:\n",
        "            return {'label': np.bincount(y).argmax()}\n",
        "\n",
        "        X_left, X_right, y_left, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
        "\n",
        "        left_tree = self.build_tree(X_left, y_left, depth + 1)\n",
        "        right_tree = self.build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return {'feature_index': feature_index, 'threshold': threshold, 'left': left_tree, 'right': right_tree}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self.build_tree(X, y)\n",
        "\n",
        "    def predict_one(self, node, x):\n",
        "        if 'label' in node:\n",
        "            return node['label']\n",
        "\n",
        "        if x[node['feature_index']] <= node['threshold']:\n",
        "            return self.predict_one(node['left'], x)\n",
        "        else:\n",
        "            return self.predict_one(node['right'], x)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.predict_one(self.tree, x) for x in X])\n",
        "\n",
        "# Train a Decision Tree\n",
        "tree = DecisionTree(max_depth=3, min_samples_leaf=2)\n",
        "tree.fit(X, y)\n",
        "\n",
        "# Predict for a new person\n",
        "new_person = np.array([[42, 2, 0, 1]])  # (Age=42, Low Income, No Student, Excellent Credit)\n",
        "prediction = tree.predict(new_person)\n",
        "print(f'Prediction for new person: {\"Buy\" if prediction[0] == 1 else \"No Buy\"}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "For8OHkSDoja"
      },
      "source": [
        "Q4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q4 a Improve the performance by bagging 10 different trees. Compute the OOB\n",
        "error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwxOC3L34zpg"
      },
      "source": [
        "a) Improve the performance by bagging 10 different trees. Compute the OOB\n",
        "error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8YBfosQDn5j",
        "outputId": "7df07767-c806-435b-bd49-7baabc178b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOB Error: 0.7500\n",
            "Prediction: No Buy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Bagging:\n",
        "    def __init__(self, base_estimator, n_estimators=10, max_depth=None, min_samples_leaf=1):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.trees = []\n",
        "        self.oob_indices = []  # Store OOB indices for each tree\n",
        "\n",
        "    def bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
        "        oob_indices = [i for i in range(n_samples) if i not in indices]\n",
        "        return X[indices], y[indices], oob_indices\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        self.oob_indices = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            X_bootstrap, y_bootstrap, oob_idx = self.bootstrap_sample(X, y)\n",
        "            tree = self.base_estimator(max_depth=self.max_depth,\n",
        "                                     min_samples_leaf=self.min_samples_leaf)\n",
        "            tree.fit(X_bootstrap, y_bootstrap)\n",
        "            self.trees.append(tree)\n",
        "            self.oob_indices.append(oob_idx)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([np.bincount(tree_preds[:, i]).argmax()\n",
        "                        for i in range(X.shape[0])])\n",
        "\n",
        "    def oob_error(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        oob_preds = np.zeros(n_samples)\n",
        "        oob_counts = np.zeros(n_samples)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            preds = []\n",
        "            for j, tree in enumerate(self.trees):\n",
        "                if i in self.oob_indices[j]:  # If sample was OOB for this tree\n",
        "                    preds.append(tree.predict(X[i:i+1])[0])\n",
        "\n",
        "            if len(preds) > 0:\n",
        "                oob_preds[i] = np.bincount(preds).argmax()\n",
        "                oob_counts[i] = 1\n",
        "\n",
        "        return np.sum(oob_counts * (oob_preds != y)) / np.sum(oob_counts)\n",
        "\n",
        "# Initialize and train\n",
        "bagging = Bagging(base_estimator=DecisionTree, n_estimators=10,\n",
        "                 max_depth=3, min_samples_leaf=1)\n",
        "bagging.fit(X, y)\n",
        "\n",
        "# OOB Error calculation\n",
        "oob_err = bagging.oob_error(X, y)\n",
        "print(f\"OOB Error: {oob_err:.4f}\")\n",
        "\n",
        "# Prediction\n",
        "new_sample = np.array([[42, 2, 0, 1]])  # Age=42, Income=Low, Student=No, Credit=Excellent\n",
        "pred = bagging.predict(new_sample)\n",
        "print(f\"Prediction: {'Buy' if pred[0] == 1 else 'No Buy'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guADL5Ub4xFf",
        "outputId": "bee2e544-b3c5-4193-8b5a-621a6b3af585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOB Error: 0.3750\n",
            "Prediction: Buy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Bagging:\n",
        "    def __init__(self, base_estimator, n_estimators=10, max_depth=None, min_samples_leaf=1):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.trees = []\n",
        "        self.oob_indices = []  # Store OOB indices for each tree\n",
        "\n",
        "    def bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
        "        oob_indices = [i for i in range(n_samples) if i not in indices]\n",
        "        return X[indices], y[indices], oob_indices\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        self.oob_indices = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            X_bootstrap, y_bootstrap, oob_idx = self.bootstrap_sample(X, y)\n",
        "            tree = self.base_estimator(max_depth=self.max_depth,\n",
        "                                     min_samples_leaf=self.min_samples_leaf)\n",
        "            tree.fit(X_bootstrap, y_bootstrap)\n",
        "            self.trees.append(tree)\n",
        "            self.oob_indices.append(oob_idx)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([np.bincount(tree_preds[:, i]).argmax()\n",
        "                        for i in range(X.shape[0])])\n",
        "\n",
        "    def oob_error(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        oob_preds = np.zeros(n_samples)\n",
        "        oob_counts = np.zeros(n_samples)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            preds = []\n",
        "            for j, tree in enumerate(self.trees):\n",
        "                if i in self.oob_indices[j]:  # If sample was OOB for this tree\n",
        "                    preds.append(tree.predict(X[i:i+1])[0])\n",
        "\n",
        "            if len(preds) > 0:\n",
        "                oob_preds[i] = np.bincount(preds).argmax()\n",
        "                oob_counts[i] = 1\n",
        "\n",
        "        return np.sum(oob_counts * (oob_preds != y)) / np.sum(oob_counts)\n",
        "\n",
        "# Initialize and train\n",
        "bagging = Bagging(base_estimator=DecisionTree, n_estimators=10,\n",
        "                 max_depth=3, min_samples_leaf=1)\n",
        "bagging.fit(X, y)\n",
        "\n",
        "# OOB Error calculation\n",
        "oob_err = bagging.oob_error(X, y)\n",
        "print(f\"OOB Error: {oob_err:.4f}\")\n",
        "\n",
        "# Prediction\n",
        "new_sample = np.array([[42, 2, 0, 1]])  # Age=42, Income=Low, Student=No, Credit=Excellent\n",
        "pred = bagging.predict(new_sample)\n",
        "print(f\"Prediction: {'Buy' if pred[0] == 1 else 'No Buy'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZyPvNf45qw"
      },
      "source": [
        "b) â€¢ Improve the performance by bagging 10 different trees but using only two\n",
        "random predictors while building the trees. Compute the OOB error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b Improve the performance by bagging 10 different trees but using only two\n",
        "random predictors while building the trees. Compute the OOB error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTyJtK8y6pya",
        "outputId": "d22124b6-fe6b-4463-ccd8-56d225c9cad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOB Error (with 2 random features per tree): 0.1250\n",
            "Prediction: Buy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "class RandomDecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_leaf=1, max_features=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.tree = None\n",
        "\n",
        "    def gini_impurity(self, y):\n",
        "        \"\"\"Calculate Gini impurity for classification.\"\"\"\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0\n",
        "        return 1 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"Find the best feature and threshold to split on.\"\"\"\n",
        "        m, n = X.shape\n",
        "        best_gini = float('inf')\n",
        "        best_split = None\n",
        "\n",
        "        # If max_features not specified, use all features\n",
        "        feature_indices = np.random.choice(\n",
        "            n,\n",
        "            size=self.max_features if self.max_features else n,\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        for feature in feature_indices:\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "                y_left, y_right = y[left_mask], y[right_mask]\n",
        "\n",
        "                # Skip invalid splits\n",
        "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Weighted Gini impurity\n",
        "                gini = (len(y_left) * self.gini_impurity(y_left) + len(y_right) * self.gini_impurity(y_right)) / m\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_split = (feature, threshold)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively build the decision tree.\"\"\"\n",
        "        # Stopping conditions\n",
        "        if len(np.unique(y)) == 1:  # All same class\n",
        "            return y[0]\n",
        "        if len(y) <= self.min_samples_leaf:\n",
        "            return np.argmax(np.bincount(y))  # Majority class\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return np.argmax(np.bincount(y))  # Majority class\n",
        "\n",
        "        split = self.best_split(X, y)\n",
        "        if split is None:  # No valid split found\n",
        "            return np.argmax(np.bincount(y))  # Majority class\n",
        "\n",
        "        feature, threshold = split\n",
        "        left_mask = X[:, feature] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_tree = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_tree = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "\n",
        "        return (feature, threshold, left_tree, right_tree)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the tree.\"\"\"\n",
        "        self.tree = self.build_tree(X, y)\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        \"\"\"Predict a single sample.\"\"\"\n",
        "        node = self.tree\n",
        "        while isinstance(node, tuple):  # While not a leaf node\n",
        "            feature, threshold, left_tree, right_tree = node\n",
        "            if x[feature] <= threshold:\n",
        "                node = left_tree\n",
        "            else:\n",
        "                node = right_tree\n",
        "        return node\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict multiple samples.\"\"\"\n",
        "        return np.array([self.predict_one(x) for x in X])\n",
        "\n",
        "class BaggingWithRandomFeatures:\n",
        "    def __init__(self, base_estimator, n_estimators=10, max_depth=None, min_samples_leaf=1, max_features=2):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "        self.oob_indices = []  # Track OOB samples for each tree\n",
        "\n",
        "    def bootstrap_sample(self, X, y):\n",
        "        \"\"\"Generate bootstrap sample and track OOB indices.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        oob_indices = [i for i in range(n_samples) if i not in indices]\n",
        "        return X[indices], y[indices], oob_indices\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the ensemble.\"\"\"\n",
        "        self.trees = []\n",
        "        self.oob_indices = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            X_boot, y_boot, oob_idx = self.bootstrap_sample(X, y)\n",
        "            tree = self.base_estimator(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                max_features=self.max_features\n",
        "            )\n",
        "            tree.fit(X_boot, y_boot)\n",
        "            self.trees.append(tree)\n",
        "            self.oob_indices.append(oob_idx)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict using majority voting.\"\"\"\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([np.bincount(tree_preds[:, i]).argmax() for i in range(X.shape[0])])\n",
        "\n",
        "    def oob_error(self, X, y):\n",
        "        \"\"\"Compute OOB error using stored OOB indices.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        oob_preds = np.zeros(n_samples)\n",
        "        oob_counts = np.zeros(n_samples)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            preds = []\n",
        "            for j, tree in enumerate(self.trees):\n",
        "                if i in self.oob_indices[j]:  # If sample was OOB for this tree\n",
        "                    preds.append(tree.predict(X[i:i+1])[0])\n",
        "\n",
        "            if len(preds) > 0:\n",
        "                oob_preds[i] = np.bincount(preds).argmax()\n",
        "                oob_counts[i] = 1\n",
        "\n",
        "        return np.sum(oob_counts * (oob_preds != y)) / np.sum(oob_counts)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "bagging_model = BaggingWithRandomFeatures(\n",
        "    base_estimator=RandomDecisionTree,\n",
        "    n_estimators=10,\n",
        "    max_depth=3,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=2\n",
        ")\n",
        "bagging_model.fit(X, y)\n",
        "\n",
        "# Compute OOB error\n",
        "oob_error = bagging_model.oob_error(X, y)\n",
        "print(f\"OOB Error (with 2 random features per tree): {oob_error:.4f}\")\n",
        "\n",
        "# Predict for a new sample\n",
        "new_sample = np.array([[42, 2, 0, 1]])  # Age=42, Income=Low, Student=No, Credit=Excellent\n",
        "pred = bagging_model.predict(new_sample)\n",
        "print(f\"Prediction: {'Buy' if pred[0] == 1 else 'No Buy'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFQulc618ym7",
        "outputId": "626786ec-849c-4963-ecfb-4c69e3a4c9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOB Error (with 2 random features per tree): 0.3750\n",
            "Prediction: No Buy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "class RandomDecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_leaf=1, max_features=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.tree = None\n",
        "\n",
        "    def gini_impurity(self, y):\n",
        "        \"\"\"Calculate Gini impurity for classification.\"\"\"\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0\n",
        "        return 1 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"Find the best feature and threshold to split on.\"\"\"\n",
        "        m, n = X.shape\n",
        "        best_gini = float('inf')\n",
        "        best_split = None\n",
        "\n",
        "        # If max_features not specified, use all features\n",
        "        feature_indices = np.random.choice(\n",
        "            n,\n",
        "            size=self.max_features if self.max_features else n,\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        for feature in feature_indices:\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "                y_left, y_right = y[left_mask], y[right_mask]\n",
        "\n",
        "                # Skip invalid splits\n",
        "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Weighted Gini impurity\n",
        "                gini = (len(y_left) * self.gini_impurity(y_left) + len(y_right) * self.gini_impurity(y_right)) / m\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_split = (feature, threshold)\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively build the decision tree.\"\"\"\n",
        "        # Stopping conditions\n",
        "        if len(np.unique(y)) == 1:  # All same class\n",
        "            return y[0]\n",
        "        if len(y) <= self.min_samples_leaf:\n",
        "            return np.argmax(np.bincount(y))  # Majority class\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return np.argmax(np.bincount(y))  # Majority class\n",
        "\n",
        "        split = self.best_split(X, y)\n",
        "        if split is None:  # No valid split found\n",
        "            return np.argmax(np.bincount(y))  # Majority class\n",
        "\n",
        "        feature, threshold = split\n",
        "        left_mask = X[:, feature] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_tree = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_tree = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "\n",
        "        return (feature, threshold, left_tree, right_tree)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the tree.\"\"\"\n",
        "        self.tree = self.build_tree(X, y)\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        \"\"\"Predict a single sample.\"\"\"\n",
        "        node = self.tree\n",
        "        while isinstance(node, tuple):  # While not a leaf node\n",
        "            feature, threshold, left_tree, right_tree = node\n",
        "            if x[feature] <= threshold:\n",
        "                node = left_tree\n",
        "            else:\n",
        "                node = right_tree\n",
        "        return node\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict multiple samples.\"\"\"\n",
        "        return np.array([self.predict_one(x) for x in X])\n",
        "\n",
        "class BaggingWithRandomFeatures:\n",
        "    def __init__(self, base_estimator, n_estimators=10, max_depth=None, min_samples_leaf=1, max_features=2):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "        self.oob_indices = []  # Track OOB samples for each tree\n",
        "\n",
        "    def bootstrap_sample(self, X, y):\n",
        "        \"\"\"Generate bootstrap sample and track OOB indices.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        oob_indices = [i for i in range(n_samples) if i not in indices]\n",
        "        return X[indices], y[indices], oob_indices\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the ensemble.\"\"\"\n",
        "        self.trees = []\n",
        "        self.oob_indices = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            X_boot, y_boot, oob_idx = self.bootstrap_sample(X, y)\n",
        "            tree = self.base_estimator(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                max_features=self.max_features\n",
        "            )\n",
        "            tree.fit(X_boot, y_boot)\n",
        "            self.trees.append(tree)\n",
        "            self.oob_indices.append(oob_idx)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict using majority voting.\"\"\"\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array([np.bincount(tree_preds[:, i]).argmax() for i in range(X.shape[0])])\n",
        "\n",
        "    def oob_error(self, X, y):\n",
        "        \"\"\"Compute OOB error using stored OOB indices.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        oob_preds = np.zeros(n_samples)\n",
        "        oob_counts = np.zeros(n_samples)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            preds = []\n",
        "            for j, tree in enumerate(self.trees):\n",
        "                if i in self.oob_indices[j]:  # If sample was OOB for this tree\n",
        "                    preds.append(tree.predict(X[i:i+1])[0])\n",
        "\n",
        "            if len(preds) > 0:\n",
        "                oob_preds[i] = np.bincount(preds).argmax()\n",
        "                oob_counts[i] = 1\n",
        "\n",
        "        return np.sum(oob_counts * (oob_preds != y)) / np.sum(oob_counts)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "bagging_model = BaggingWithRandomFeatures(\n",
        "    base_estimator=RandomDecisionTree,\n",
        "    n_estimators=10,\n",
        "    max_depth=3,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=2\n",
        ")\n",
        "bagging_model.fit(X, y)\n",
        "\n",
        "# Compute OOB error\n",
        "oob_error = bagging_model.oob_error(X, y)\n",
        "print(f\"OOB Error (with 2 random features per tree): {oob_error:.4f}\")\n",
        "\n",
        "# Predict for a new sample\n",
        "new_sample = np.array([[42, 2, 0, 1]])  # Age=42, Income=Low, Student=No, Credit=Excellent\n",
        "pred = bagging_model.predict(new_sample)\n",
        "print(f\"Prediction: {'Buy' if pred[0] == 1 else 'No Buy'}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
